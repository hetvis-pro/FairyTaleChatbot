{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Required Packages\n",
        "\n",
        "- `langchain`: The core library for creating LLM-based applications. It offers tools for document loading, splitting, embedding, vector storage, memory, and more.\n",
        "- `langchain-community`: Contains community-maintained integrations for document loaders, vector stores, and models.\n",
        "- `langchain-mistralai`: Provides access to **Mistral LLMs** via LangChain, enabling us to use Mistral as our language model backend.\n",
        "- `chromadb`: A lightweight and fast vector database used to store and retrieve document embeddings efficiently. It's a key component of our retrieval system.\n",
        "- `gradio`: A UI library that allows us to build an interactive web interface where users can type questions and receive AI-generated answers from our fairy tale chatbot.\n",
        "\n",
        "By installing these packages, weâ€™re setting up the software environment to support every core function of the RAG architecture: **data ingestion, preprocessing, embedding, retrieval, generation, and user interaction**.\n"
      ],
      "metadata": {
        "id": "O-nZAu2m8Dp6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "g1lvhC-AJZ5q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51654452-bb56-4851-8485-c3c12fe8d702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain-community langchain-mistralai chromadb gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Required Modules\n",
        "\n",
        "This cell imports all the core libraries and components needed to build the Retrieval-Augmented Generation (RAG) chatbot pipeline.\n",
        "\n",
        "\n",
        "- `PyPDFLoader`:  \n",
        "  Loads and parses PDF files. This is used to ingest the fairy tale documents into the system.\n",
        "\n",
        "- `RecursiveCharacterTextSplitter`:  \n",
        "  Splits long text documents into smaller, overlapping chunks. This improves retrieval quality and helps ensure that each chunk is within the token limit for embedding.\n",
        "\n",
        "- `Chroma`:  \n",
        "  A vector database used to store and retrieve text embeddings. It performs similarity searches when a user asks a question.\n",
        "\n",
        "- `HuggingFaceEmbeddings`:  \n",
        "  Transforms text into vector embeddings using a transformer-based model from Hugging Face.\n",
        "\n",
        "- `ConversationBufferMemory`:  \n",
        "  Maintains the conversation history, allowing the chatbot to respond with context-aware answers over multiple turns.\n",
        "\n",
        "- `ConversationalRetrievalChain`:  \n",
        "  A LangChain component that combines document retrieval and LLM-based answer generation in a single chain.\n",
        "\n",
        "- `ChatMistralAI`:  \n",
        "  Provides an interface to the Mistral large language model via API, which is used to generate answers to user queries.\n",
        "\n",
        "- `gradio`:  \n",
        "  A Python library for building interactive web UIs. This will be used to create the chatbot interface.\n",
        "\n",
        "- `os`:  \n",
        "  Standard Python module used here to handle environment variables, such as setting the API key securely.\n",
        "\n",
        "These components form the foundational building blocks of the RAG system: loading, splitting, embedding, storing, retrieving, and responding to user queries.\n"
      ],
      "metadata": {
        "id": "HrlWqZxk-DId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "import gradio as gr\n",
        "import os"
      ],
      "metadata": {
        "id": "DyckJZ_TJhIe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting the Mistral API Key and Initializing the Language Model\n",
        "\n",
        "This cell configures access to the Mistral language model by setting the required API key and initializing the model.\n",
        "\n",
        "\n",
        "- `os.environ[\"MISTRAL_API_KEY\"] = \"key\"`  \n",
        "  This line sets the `MISTRAL_API_KEY` environment variable, which is used to authenticate requests made to the Mistral API.  \n",
        "  In production or shared environments, the actual key should be stored securely and not hardcoded.\n",
        "\n",
        "- `ChatMistralAI(model=\"mistral-small\", temperature=0)`  \n",
        "  Initializes the Mistral language model with the specified parameters:\n",
        "  - `model=\"mistral-small\"` refers to the specific model variant being used.\n",
        "  - `temperature=0` sets the randomness of the output to zero, making the modelâ€™s responses more deterministic and consistent.\n",
        "\n",
        "This model will be used later in the pipeline to generate natural language responses based on the userâ€™s query and the retrieved document content.\n"
      ],
      "metadata": {
        "id": "ypDDd40r_Lv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"MISTRAL_API_KEY\"] = \"ZYxgsoPPGqJYjHIGoCIEbP1vcxbNJstX\"\n",
        "mistral_llm = ChatMistralAI(model=\"mistral-small\", temperature=0)\n"
      ],
      "metadata": {
        "id": "f_fIe7vpKIjm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing PDF Parser and Loading Documents\n",
        "\n",
        "This cell performs two key actions: installing a PDF parsing library and loading the fairy tale PDF documents from a directory.\n",
        "\n",
        "\n",
        "\n",
        "- `!pip install pypdf`  \n",
        "  Installs the `pypdf` library, which is a dependency for parsing and reading PDF files. This is required for LangChainâ€™s PDF loaders to function correctly.\n",
        "\n",
        "- `from langchain_community.document_loaders import PyPDFDirectoryLoader`  \n",
        "  Imports the `PyPDFDirectoryLoader` class, which allows batch loading of all PDF files from a specified folder.\n",
        "\n",
        "- `loader = PyPDFDirectoryLoader(\"/content\")`  \n",
        "  Creates a document loader instance targeting the `/content` directory (default working directory in Google Colab). All PDF files placed in this folder will be read.\n",
        "\n",
        "- `documents = loader.load()`  \n",
        "  Loads and parses the PDF files into a list of LangChain `Document` objects. Each document contains:\n",
        "  - The textual content extracted from the PDF\n",
        "  - Metadata such as the file name and page number\n",
        "\n",
        "These documents will later be split into chunks, embedded into vectors, and stored in a vector database for retrieval during chatbot interactions.\n"
      ],
      "metadata": {
        "id": "l4K_rIrV_3pC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-d8Z0RzKvY0",
        "outputId": "2ecbe9a1-c0a9-485d-f4c7-2ca65b5e2349",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/302.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "loader = PyPDFDirectoryLoader(\"/content\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "Vhz7Osb2Kdqn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting Documents into Chunks\n",
        "\n",
        "This cell splits the loaded documents into smaller text chunks using LangChainâ€™s text splitter utility. Splitting is essential for efficient embedding and retrieval in a RAG pipeline.\n",
        "\n",
        "- `RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)`  \n",
        "  Initializes a text splitter that breaks large documents into smaller segments of up to 500 characters each.  \n",
        "  The `chunk_overlap=50` means that 50 characters from the end of one chunk are repeated at the start of the next chunk.  \n",
        "  This overlapping technique helps preserve context and avoids cutting off important information at chunk boundaries.\n",
        "\n",
        "- `docs = splitter.split_documents(documents)`  \n",
        "  Applies the text splitter to the previously loaded `documents`.  \n",
        "  The result is a list of smaller, manageable text chunks stored in `docs`. Each chunk retains the original documentâ€™s metadata.\n",
        "\n",
        "Splitting documents is a critical preprocessing step in RAG systems. It ensures that:\n",
        "- The input size fits within token limits of embedding models and LLMs\n",
        "- Retrieval is more fine-grained and contextually relevant\n"
      ],
      "metadata": {
        "id": "lRTcTgS2BlWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "UbC1lAVHKlMj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating Embeddings and Initializing Vector Store\n",
        "\n",
        "This cell sets up the document embedding model and initializes the vector database for efficient semantic search.\n",
        "\n",
        "\n",
        "We use `HuggingFaceEmbeddings` with the pre-trained model `\"all-MiniLM-L6-v2\"` to convert each chunk of text into a dense numerical vector. This model is widely used due to its excellent trade-off between speed and semantic performance. It captures sentence-level meaning and is lightweight enough for real-time inference on consumer-grade hardware. Hugging Face models also run locally and are open-source, making them cost-effective and flexible for academic and prototype projects.\n",
        "\n",
        "The vector store is implemented using `Chroma`, which supports fast in-memory and persistent storage of vector data. `Chroma.from_documents()` takes the list of preprocessed document chunks and their embeddings, storing them internally for future retrieval. Finally, we extract a `retriever` object from the vector store using `.as_retriever()`, which enables semantic similarity search based on user queries.\n",
        "\n",
        "This approach ensures that when a user asks a question, the system can find and retrieve the most relevant text chunks from the embedded knowledge base using cosine similarity.\n"
      ],
      "metadata": {
        "id": "NuN1anT5s2Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "vectordb = Chroma.from_documents(docs, embedding=embedding)\n",
        "retriever = vectordb.as_retriever()"
      ],
      "metadata": {
        "id": "8egmjAK8K1kN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570,
          "referenced_widgets": [
            "6e18a4cbb0aa46d5b3233cb8b0298d95",
            "0956e394289d4fb78c8e7210cc901cb5",
            "ced1a1d7750c4a8ca59038e9b38b06c7",
            "389b0418a91a4147ba8d503e336ec08c",
            "b45eed7a181a44de9cecbb70bfb85852",
            "a5a1ee3e7aad4b3dbe1312e2f5f9463f",
            "5f5b950c571e44958fb2bb2e4c73ab1a",
            "4aa6bf4e9b6e41b982ed3a0b75189c3d",
            "82bf04572bfa4f898d08ac11513cd416",
            "d520fca4db974727a4e21bcfe5b7ee81",
            "0884a1182b054889934642d01dd06529",
            "cdb282be887a483183ef4d1c82a9a180",
            "b9819ba1d0d84af295b9293486123775",
            "6b579237d79540948a445fc69658aa1b",
            "5cbfbad887864164a90add5f22e4642f",
            "1d716a3506e549cbbd9c163b5325209c",
            "1b5db6b7f1634670b5fb676266a057b7",
            "d27fbdc6f0ac496e8022b454e3b639c4",
            "09e5f9a4d09244589e6a678aa407d8e4",
            "53b6e941d1724f318655c502df32c328",
            "6443b066bde64a5b88a7c7e0e25bb435",
            "5ad17c95737642afaf25c117aa22a7b7",
            "8973da3fe5b6462c9b29734ae5832ad1",
            "7f52d8bb4c214736a326e335e23f8a97",
            "8b9f2c6235f540d1b640735aa9763aa7",
            "41e7c14c780947088f5fb987e0c642be",
            "b716818b881442ac9fc3e2f0d7c470e9",
            "1a5dd560a46c4ae3ac5bc51f0860656e",
            "d93fe94b435245fea973d84c584b86fa",
            "c4e843bffbe94924925b69eeb44c7fcd",
            "9eac327f5c474056b46f48aba03ebdf9",
            "4f79709aaab84b0f940cea94ab1170cf",
            "9121ea58c9a34011aeef6b3d92d134fb",
            "3c4fea283bce4299bc341f44e98b79cd",
            "2a27ed118d04413789f3873f144a89d7",
            "d8dd84ffcaec4f24be1ebf48092a5406",
            "2dab61644446426b84bbd003897d6546",
            "b94ed40487514ddd8619f44e166abb24",
            "4dfe65515998415cab8859bdf77ab4d6",
            "800600e7dc744eccb28530fccaba5b47",
            "5689084df44b4c7eb5d65caf6a83a834",
            "659413c744c34fe8a6adc9f1898f9644",
            "03f479d2b12e4bbc995d8d81464e4859",
            "c0cafc5a13a24eabaaafc2c5bcf1f432",
            "557bfd5682164280bb8be02faff88bac",
            "4bb394aa0fb94811861f828ed22414eb",
            "b09ae8aa46e74854bf42f79307aefd0a",
            "c1c239eebc904d1ca114dc8d2ec906d6",
            "0e6b455ec1dd468085cfac8c352ae40d",
            "b89935c1f01d451aae18f32c2d53a585",
            "8d0325882a1d4c97a40bfaa9211aa556",
            "5e56b65d077e4656a142ceb1333e2aef",
            "e8a16ea0cdb043c8b6ad63fbcbc6f1a7",
            "97c46548343d49f08e134edcc354eb46",
            "9ccdb3f726bf4519aeac8dcbe8f3b0fa",
            "fa5a313358614f2fbc68008946b4dbaf",
            "601479b1f0ea44bebbe45e877c252d88",
            "ed4d62bee3ec4d4e8357cb84a0a05183",
            "85b3d911b78b49f9bcb568916bb8766b",
            "1d3bbfd48ac84e35a96faf71b5e2051f",
            "34a6bf282e6448398d72839b4b2bf495",
            "f94ba4d4d16547efb27628957deee477",
            "9913c14596df4826b521a3cbc9a3405e",
            "85dab94a03eb41bd94064da1f350448e",
            "8acf8daa056d472e8e8e41db6f33288c",
            "ccaf8411cd90427d880ac9730b7a71b8",
            "f5a4004aeeee4d67890e414bacbef9c5",
            "ec599057c74544799f07fa2b81599068",
            "1302be6a4da74ca1a28e2df63de1cf68",
            "39f14887de6d4525a672d5920a36d4a5",
            "546b6efde37944fc941f16fc657625d0",
            "22c494a542054c559b2194606bdfe789",
            "4ce7929a44514d14b77cf7906cc5e88f",
            "c7709a0982254e12a927da62395af242",
            "acdf624026fe496696b49dae3b84298c",
            "98e5e06c9a2f4806a95160d8fdaf7815",
            "2e7d2429c32b44a3b03705ffc279d53c",
            "086b45eec6bb4eceb0aab01f8a688456",
            "eb97b6409ba64ff0b474ac6669a8b68e",
            "afe59fc5951a432aba7e7ecc34e6659e",
            "04fcb12cd4924045a9eea1eb411da817",
            "dd25467cdf4e4d4da66ed830b14254e3",
            "e92e9ba42781432196d91ded14540ac3",
            "7d0e66e808c54cae97a24578dc5f6af7",
            "2d3c5fe15b5d45179eb96a8d42b4a6f0",
            "c40d9dcc2f5e42e39f9e8529e3c68710",
            "f58aa4fc408243c7aadcfa82dc19e7d3",
            "2a175ca627014b9999ef1c56cf30f119",
            "d0007db186a442c4bf25c189132ac6f1",
            "5fb44cff7f4b4537bc06d152b0abd2bc",
            "94cce2cb21c64d24aaafca365eb79487",
            "67d026442ef64c5bacdde215ea9b0df3",
            "f41a59c48e1148e188915c1c990f56e3",
            "93a1785ddf914731a9f98663bee9752d",
            "0b0a74ddbe924c5093a81d01f9d8329e",
            "a47033a91a1147f997b33543b0f74ced",
            "e29dcade0ae54a2b8bedf3bd698c4cee",
            "f7892b31fae44c4cb207307e6d825f8b",
            "09013e6d7be849cba52335cbc8e54d4f",
            "c8233773063a4a5ab86f2145045a50bb",
            "54eed3b260a742119e9b30db67e81be0",
            "c6a05bd3b0674161896b021872df8876",
            "4c6821d5a8654f6c9754237531b608ce",
            "b5d8b9d57fa64548b215fb764f4ecdc8",
            "40221e75b44c4cc9a2b426a0ed03f2a7",
            "59efd3b18b6a484487117a06f5ebe566",
            "af6e2c36aa3449c79940eecfff2e8d07",
            "fd5e1b68e8824d1dbb2ad7724651294a",
            "e3047f16093e4df29aab7eca0f58aba4",
            "444f1292292f4dfc9a9e18f59fff7f9d",
            "04ac941cc4344ed4a0ae140eada46733",
            "1054e377d58b4070bd3d9cb499d0ad18",
            "be36973de5f64d7c884ec212032d3326",
            "94747551a7c8470d8e0f4e8b227783a7",
            "6e7ebc9d25e44a34aaf60be3ad9d69e1",
            "a657ab8ee16f4bd7a01e7052a7f99fdd",
            "de5ed8ae4ea54aecb503fa1d9a804935",
            "0e67168758fd4572b2f858e75f57e57d",
            "d029af7084a949008f5674372d6180ab",
            "186dfd5a494d4612bcd78749aa0091de",
            "8e3c5c0867e94dcc8dd5bf835fdaa2c6"
          ]
        },
        "outputId": "32040082-7407-43dc-efe5-e30ed4f91919"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-fe6f70b0ecb2>:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e18a4cbb0aa46d5b3233cb8b0298d95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdb282be887a483183ef4d1c82a9a180"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8973da3fe5b6462c9b29734ae5832ad1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c4fea283bce4299bc341f44e98b79cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "557bfd5682164280bb8be02faff88bac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa5a313358614f2fbc68008946b4dbaf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5a4004aeeee4d67890e414bacbef9c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "086b45eec6bb4eceb0aab01f8a688456"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0007db186a442c4bf25c189132ac6f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8233773063a4a5ab86f2145045a50bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04ac941cc4344ed4a0ae140eada46733"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Initializing Conversation Memory\n",
        "\n",
        "This cell sets up memory management for the chatbot using LangChain's `ConversationBufferMemory`. Memory is essential for maintaining the context of multi-turn conversations.\n",
        "\n",
        "\n",
        "\n",
        "- `ConversationBufferMemory(...)`  \n",
        "  Creates a memory object that stores the full conversation history in memory, enabling the chatbot to respond with awareness of prior exchanges. It helps produce more coherent and context-aware answers.\n",
        "\n",
        "#### Parameters:\n",
        "- `memory_key=\"chat_history\"`  \n",
        "  Defines the key used to store and retrieve past dialogue messages from memory.\n",
        "\n",
        "- `return_messages=True`  \n",
        "  Ensures that past messages are returned in their original message format (rather than raw text), which is required by some retriever chains or LLM input formats.\n",
        "\n",
        "- `output_key=\"answer\"`  \n",
        "  Specifies that the output of the retrieval-augmented generation pipeline will be stored under the key `\"answer\"` in memory.\n",
        "\n",
        "Using memory in RAG applications enhances the chatbotâ€™s ability to hold meaningful conversations over multiple turns rather than responding in isolation.\n"
      ],
      "metadata": {
        "id": "3vOqjovhQEH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        "    output_key=\"answer\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "c-shm1HHK3bU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f0462b-cec1-4c8c-a4fb-9d6cb07c89ff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-bdcf5d2ff166>:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Conversational Retrieval Chain\n",
        "\n",
        "This cell sets up the core of the Retrieval-Augmented Generation (RAG) system using LangChainâ€™s `ConversationalRetrievalChain`. This chain combines document retrieval with language model generation in a single, seamless interface.\n",
        "\n",
        "- `ConversationalRetrievalChain.from_llm(...)`  \n",
        "  This method initializes a conversation-aware QA system that can:\n",
        "  1. Retrieve relevant document chunks from the vector store based on the current user query\n",
        "  2. Combine that with prior conversation history (via memory)\n",
        "  3. Pass everything to the language model to generate a well-informed response\n",
        "\n",
        "#### Parameters:\n",
        "\n",
        "- `llm=mistral_llm`  \n",
        "  Specifies the language model to use for generating answers. In this case, it's the previously initialized Mistral model (`mistral-small`).\n",
        "\n",
        "- `retriever=retriever`  \n",
        "  Connects the retriever (typically a Chroma-based similarity search tool) that fetches relevant document chunks based on the userâ€™s query.\n",
        "\n",
        "- `memory=memory`  \n",
        "  Injects the conversation memory created earlier. This allows the chain to handle multi-turn conversations and maintain context from earlier interactions.\n",
        "\n",
        "- `return_source_documents=True`  \n",
        "  Ensures that the original document chunks used for generating the answer are returned along with the answer itself. This is useful for transparency or debugging.\n",
        "\n",
        "- `output_key=\"answer\"`  \n",
        "  Defines the key under which the generated response will be stored and accessed.\n",
        "\n",
        "This chain becomes the brain of the chatbot, managing context, performing retrieval, and generating responses.\n"
      ],
      "metadata": {
        "id": "Z43x0ilJQep8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=mistral_llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    return_source_documents=True,\n",
        "    output_key=\"answer\"\n",
        ")"
      ],
      "metadata": {
        "id": "XdQ0HuRvLH7W"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Gradio Chatbot Interface\n",
        "\n",
        "This cell defines the backend logic for the chatbotâ€™s interactive user interface using Gradio. It handles user messages, determines appropriate responses, and invokes the RAG model to generate answers.\n",
        "\n",
        "\n",
        "- `respond_to_user(message, history)`  \n",
        "  This is the main callback function that gets triggered whenever a user sends a message through the Gradio chat interface. It processes the message, interacts with the QA pipeline, and returns the response.\n",
        "\n",
        "### Key Functional Steps:\n",
        "\n",
        "1. **Greeting and Exit Handling:**\n",
        "   - The input message is converted to lowercase and stripped of extra spaces.\n",
        "   - If the message is a greeting (like â€œhiâ€, â€œhelloâ€, etc.), a friendly welcome message is returned.\n",
        "   - If the user wants to exit (e.g., â€œbyeâ€, â€œgoodbyeâ€), a farewell message is shown.\n",
        "   - These are handled before invoking the QA chain.\n",
        "\n",
        "2. **Answer Retrieval:**\n",
        "   - The `qa_chain.invoke({\"question\": message})` call sends the user's message to the Conversational Retrieval Chain.\n",
        "   - The chain returns a dictionary with keys like `\"answer\"` or `\"result\"`, depending on the LLM used.\n",
        "   - The code safely retrieves the response using `.get()` to avoid key errors.\n",
        "\n",
        "3. **Fallback Handling:**\n",
        "   - If the answer indicates uncertainty (e.g., contains phrases like â€œdonâ€™t knowâ€ or â€œnot sureâ€), a soft, encouraging fallback message is added.\n",
        "   - This keeps the user experience positive, even if the chatbot cannot find a good answer.\n",
        "\n",
        "4. **Exception Handling:**\n",
        "   - If an error occurs during execution, the traceback is printed for debugging, and a formatted error message is returned to the UI.\n",
        "\n",
        "This function enables dynamic interaction between users and the RAG model, supporting real-time Q&A with conversational memory and fallback safety.\n"
      ],
      "metadata": {
        "id": "edoN8j4CQ7S9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def respond_to_user(message, history):\n",
        "    try:\n",
        "        message_lower = message.lower().strip()\n",
        "        greetings = [\"hi\", \"hello\", \"hey\", \"good morning\", \"good evening\", \"what's up\", \"how are you\"]\n",
        "        exit_phrases = [\"bye\", \"goodbye\", \"see you later\", \"exit\"]\n",
        "\n",
        "        if message_lower in greetings:\n",
        "            return \"ğŸ§šâ€â™€ï¸ Hello! Ask me anything about fairy tales and Iâ€™ll do my best to help!\"\n",
        "        if message_lower in exit_phrases:\n",
        "            return \"ğŸ‘‹ Bye! Have a magical day! ğŸŒŸ\"\n",
        "\n",
        "        response = qa_chain.invoke({\"question\": message})\n",
        "        answer = response.get(\"answer\", \"\") or response.get(\"result\", \"\")\n",
        "\n",
        "        if \"don't know\" in answer.lower() or \"not sure\" in answer.lower():\n",
        "          answer += \" ğŸ˜Š I'm sorry, I don't know the answer to this question. But I'm always learning!\"\n",
        "\n",
        "        return answer + \" \\nğŸ§™â€â™€ï¸Thanks for asking!\\n Do you want to ask anything else?\"\n",
        "\n",
        "        #return response[\"answer\"]\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "        return f\"Error:\\n{str(e)}\"\n"
      ],
      "metadata": {
        "id": "IO0MY5JiLIi6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Gradio UI for the Fairy Tale Chatbot\n",
        "\n",
        "This cell builds a user-friendly web interface using `gradio.Blocks` for interacting with the fairy tale RAG chatbot. It includes a visual header, an optional image, and a conversational chat interface that calls the backend function `respond_to_user()`.\n",
        "\n",
        "\n",
        "- `with gr.Blocks(theme=gr.themes.Soft()) as demo:`  \n",
        "  Initializes a Gradio Blocks interface with a clean and modern visual theme. The `Soft` theme provides a light, user-friendly design.\n",
        "\n",
        "- `gr.Markdown(...)`  \n",
        "  Adds custom Markdown text to the interface:\n",
        "  - The first line serves as a title.\n",
        "  - The second line introduces the chatbot and what users can expect from it.\n",
        "\n",
        "- `gr.Image(\"/content/bg.gif\", height=278, width=500)`  \n",
        "  Displays a background or thematic image (e.g., a fairy-tale themed GIF). This makes the UI visually appealing and engaging for users.\n",
        "\n",
        "- `gr.ChatInterface(...)`  \n",
        "  Builds the core chat module using Gradioâ€™s high-level chat wrapper. Key attributes include:\n",
        "  - `fn=respond_to_user`: Connects the userâ€™s message input to the chatbot response function.\n",
        "  - `title`: Sets the chat windowâ€™s title.\n",
        "  - `description`: Provides instructions or context for the user.\n",
        "  - `examples`: Offers predefined example questions users can click on to get started.\n",
        "  - `type=\"messages\"`: Formats the input and output as chat-style messages.\n",
        "\n",
        "- `demo.launch(share=True, inline=False)`  \n",
        "  Launches the Gradio app.\n",
        "  - `share=True` generates a public link so the chatbot can be accessed and tested outside of the notebook.\n",
        "  - `inline=False` ensures that the interface opens in a new browser tab instead of embedding within the notebook.\n",
        "\n",
        "This interface makes it easy for users to have natural conversations with the fairy tale chatbot without needing to interact with raw code or command-line prompts."
      ],
      "metadata": {
        "id": "-KxVpJugVA3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"## ğŸ§šâ€â™€ï¸ Welcome to Your Magical Fairy Tale Chatbot!\")\n",
        "    gr.Markdown(\"Talk to classic fairy tales like never before âœ¨ Ask about plots, characters, morals, and more.\")\n",
        "    gr.Image(\"/content/bg.gif\", height=278, width = 500)\n",
        "    gr.ChatInterface(\n",
        "        fn=respond_to_user,\n",
        "        title=\"ğŸ§š Fairy Tale RAG Chatbot\",\n",
        "        description=\"Ask anything about your favourite fairy tales!\",\n",
        "        examples=[\"Does the little mermaid sing?\", \"Who helped Rapunzel escape?\"],\n",
        "        type=\"messages\"\n",
        "        )\n",
        "demo.launch(share=True, inline=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_PrdLMELmaV",
        "outputId": "876b9c10-e5b6-4664-a71a-7abf3ec8287b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c850dd322777967d3f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iikMIqvJu5Mc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
